{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T21:46:17.265010Z",
     "start_time": "2018-04-18T21:46:11.571166Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import regex as re\n",
    "import pprint\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "#Aprori\n",
    "import sys\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "\n",
    "\n",
    "## Tokenize\n",
    "from collections import defaultdict\n",
    "import nltk.tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars\n",
    "\n",
    "\n",
    "##\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import spacy\n",
    "\n",
    "## Classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "## Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "PATH = \"./\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Summarization - Aspect based opinion mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize opinions of users about a product from a set of reviews. Extract the most common product features mentioned, most common opinion words used for a feature and the corresponding positive and negitive opinions about related to the feature and opinions. This way it becomes extremely easy to identify the most prominent positive and negitive features and opinions about a product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology:\n",
    "\n",
    "1. Tokenize the reviews into sentences.\n",
    "\n",
    "2. Create a set of nouns in each sentence. Extract the most common nouns in all of the reviews using apriori algorithm. This gives us the most frequent features/aspects of the product.\n",
    "\n",
    "3. Classify each sentence containing a product feature as positive, negitive or neutral using the Liu and Hu opinion lexicon.\n",
    "\n",
    "4. Extract opinion phrases from sentences using nltk chunking. We identify the most frequent opinion phrases using frequency distribution.\n",
    "\n",
    "5. For each product feature and corresponding opinion phrase, we display the positive and negitive reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample:\n",
    "\n",
    "**Product**: phone\n",
    "\n",
    "**Features**: camera, battery, price, performance\n",
    "\n",
    "**Opinion Phrases**: great camera, great battery, low light, heat issues\n",
    "\n",
    "**Output for a feature**:\n",
    "\n",
    "*Feature* - camera\n",
    "\n",
    "*Positive* - great camera\n",
    "\n",
    "*Reviews* - \n",
    "1. Superb slick phone, great camera nice depth mode like DSLR cameraphone is a beast n crazy fast...I am very happy with the purchase...\n",
    "2. portrail photo is the awsome feature, perfect andoid product at hisAwesome product with great camera features.. Lighting fast processor...\n",
    "3. Nice phone with great camera battery backup\n",
    "4. Amazing phone in good price,with great camera experience\n",
    "\n",
    "*Negitive* - low light\n",
    "\n",
    "*Reviews* - \n",
    "1. low lighting photos could have been better.\n",
    "2. Camera quality awesome on rear, but struggling on low lighting conditions a little bit as there sometimes appears noise in image captured in low light.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "\n",
    "We have a file containing 1000 reviews of One plus 5 from Amazon in json format.\n",
    "\n",
    "*Each review has the following details attached with it:*\n",
    "1.review_author \n",
    "2.review_header \n",
    "3.review_posted_date \n",
    "4.review_rating \n",
    "5.review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T21:51:12.188746Z",
     "start_time": "2018-04-18T21:51:12.161059Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#import configparser\n",
    "\n",
    "\n",
    "PATH_KEY = '/home/chesterhsieh/Desktop/579_proj/CS579_Project/Twiiter_Spider/private_key.txt'\n",
    "\n",
    "def get_setting(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    ls_lines = list()\n",
    "    dict_info = dict()\n",
    "    pat = r\"[\\d|\\w|\\W]+\\s=\"\n",
    "    pat_content = r\"'[\\d|\\w|\\W]+'\"\n",
    "    with open(config_file,'r') as text_handler:\n",
    "        ls_lines = text_handler.readlines()\n",
    "        for line in ls_lines:\n",
    "            dict_info[re.findall(pat,line)[0][:-2]] = re.findall(pat_content,line)[0][1:-1]\n",
    "    return dict_info\n",
    "\n",
    "dict_key =  get_setting(PATH_KEY)\n",
    "\n",
    "MONGO_HOST_TW= \"mongodb://{0}:{1}@ec2-52-87-161-70.compute-1.amazonaws.com/twitterdb\".format(dict_key['mongo_db_user'],dict_key['mongo_db_password']) # assuming you have mongoDB installed locally\n",
    "MONGO_HOST_Amazondb = \"mongodb://{0}:{1}@ec2-52-87-161-70.compute-1.amazonaws.com/Amazondb\".format(dict_key['mongo_db_user'],dict_key['mongo_db_password']) # assuming you have mongoDB installed locally\n",
    "\n",
    "client_tw = MongoClient(MONGO_HOST_TW)\n",
    "db_tw = client_tw.twitterdb\n",
    "client_a = MongoClient(MONGO_HOST_Amazondb)\n",
    "db_a = client_a.Amazondb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:16:12.316835Z",
     "start_time": "2018-04-18T23:16:06.204675Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_am = pd.DataFrame.from_records(db_a.Amazon_review.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:16:13.399115Z",
     "start_time": "2018-04-18T23:16:13.388282Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LastModificationTime</th>\n",
       "      <th>ReviewText</th>\n",
       "      <th>_id</th>\n",
       "      <th>p_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>on June 12, 2017</td>\n",
       "      <td>I absolutely love LA Girl eye shadow. The colo...</td>\n",
       "      <td>5ac7e945a785ab10007f6f53</td>\n",
       "      <td>B00PGQYEUK</td>\n",
       "      <td>5</td>\n",
       "      <td>Fantastic Product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LastModificationTime                                         ReviewText  \\\n",
       "0     on June 12, 2017  I absolutely love LA Girl eye shadow. The colo...   \n",
       "\n",
       "                        _id        p_id rating              title  \n",
       "0  5ac7e945a785ab10007f6f53  B00PGQYEUK      5  Fantastic Product  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_am.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T01:03:51.280410Z",
     "start_time": "2018-04-19T01:03:51.269827Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_for_scat = pd.concat([df_am.groupby('p_id')['rating'].mean(),df_am.groupby('p_id')['title'].count()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T02:05:25.046029Z",
     "start_time": "2018-04-19T02:05:25.044147Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_for_scat.columns = ['avg_rating','Review_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T02:05:32.987134Z",
     "start_time": "2018-04-19T02:05:32.848507Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8e98cd4048>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAELCAYAAAD3HtBMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e98df26a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_for_scat.plot.scatter(x = 'avg_rating',y ='Review_number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy treatment before all the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:16:22.866593Z",
     "start_time": "2018-04-18T23:16:14.349163Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_am[['rating']] = df_am[['rating']] .apply(pd.to_numeric)\n",
    "df_am.drop('_id',inplace= True,axis =1)\n",
    "df_am['LastModificationTime'] = df_am.LastModificationTime.apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T20:11:23.771347Z",
     "start_time": "2018-04-10T20:11:23.767408Z"
    }
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize all the reviews as sentences into sent_fullreview. Then the 1 and 2 star rated reviews are tokenized into sent_neg_review and the rest of the reviews into sent_review.\n",
    "\n",
    "I have customized the PunktSentenceTokenizer to separate sentences on a few extra words and characters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:16:23.805904Z",
     "start_time": "2018-04-18T23:16:23.782693Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for r in file_data:\n",
    "#     for s in r:\n",
    "#         reviews_final_str = reviews_final_str + str(s['review_text'])\n",
    "#         if ((s['review_rating'][0][:1]==\"1\") or (s['review_rating'][0][:1]==\"2\")):\n",
    "#             reviews_neg_str = reviews_neg_str + str(s['review_text'])\n",
    "#         else:\n",
    "#             reviews_str = reviews_final_str + str(s['review_text'])\n",
    "\n",
    "\n",
    "\n",
    "'''We customize the ReviewLangVars class to separate sentences based on some additional keywords'''\n",
    "class ReviewLangVars(PunktLanguageVars):\n",
    "    sent_end_chars = ('pros:', 'cons:', '[','][','.','?','!')\n",
    "    \n",
    "    \n",
    "sent_tokenizer1 = PunktSentenceTokenizer(lang_vars = ReviewLangVars())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:16:34.226416Z",
     "start_time": "2018-04-18T23:16:24.793655Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_am['tkn_sent'] = df_am.ReviewText.apply(sent_tokenizer1.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T20:15:24.080205Z",
     "start_time": "2018-04-10T20:15:24.075347Z"
    }
   },
   "source": [
    "## Apriori Alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract common features using Apriori algorithm.\n",
    "\n",
    "Apriori algorithm is used for frequent item set mining and association rule learning over transactional data. It proceeds by identifying the frequent individual items in the data and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the data. The frequent item sets determined by Apriori can be used to determine association rules\n",
    "\n",
    "Further details about it are mentioned in the research paper\n",
    "**Fast Algorithms for Mining Association Rules\n",
    "Rakesh Agrawal Ramakrishnan S&ant*\n",
    "IBM Almaden Research Center\n",
    "650 Harry Road, San Jose, CA 95120**\n",
    "\n",
    "The python implementaion of the code for apriori alogirthm is taken from https://github.com/asaini/Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:18:22.788895Z",
     "start_time": "2018-04-18T23:18:22.671721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])\n",
    "\n",
    "\n",
    "def returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet):\n",
    "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
    "       of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
    "        _itemSet = set()\n",
    "        localSet = defaultdict(int)\n",
    "\n",
    "        for item in itemSet:\n",
    "                for transaction in transactionList:\n",
    "                        if item.issubset(transaction):\n",
    "                                freqSet[item] += 1\n",
    "                                localSet[item] += 1\n",
    "\n",
    "        for item, count in localSet.items():\n",
    "                support = float(count)/len(transactionList)\n",
    "\n",
    "                if support >= minSupport:\n",
    "                        _itemSet.add(item)\n",
    "\n",
    "        return _itemSet\n",
    "\n",
    "\n",
    "def joinSet(itemSet, length):\n",
    "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "\n",
    "def getItemSetTransactionList(data_iterator):\n",
    "    transactionList = list()\n",
    "    itemSet = set()\n",
    "    for record in data_iterator:\n",
    "        transaction = frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))              # Generate 1-itemSets\n",
    "    return itemSet, transactionList\n",
    "\n",
    "\n",
    "def runApriori(data_iter, minSupport, minConfidence):\n",
    "    \"\"\"\n",
    "    run the apriori algorithm. data_iter is a record iterator\n",
    "    Return both:\n",
    "     - items (tuple, support)\n",
    "     - rules ((pretuple, posttuple), confidence)\n",
    "    \"\"\"\n",
    "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
    "\n",
    "    freqSet = defaultdict(int)\n",
    "    largeSet = dict()\n",
    "    # Global dictionary which stores (key=n-itemSets,value=support)\n",
    "    # which satisfy minSupport\n",
    "\n",
    "    assocRules = dict()\n",
    "    # Dictionary which stores Association Rules\n",
    "\n",
    "    oneCSet = returnItemsWithMinSupport(itemSet,\n",
    "                                        transactionList,\n",
    "                                        minSupport,\n",
    "                                        freqSet)\n",
    "\n",
    "    currentLSet = oneCSet\n",
    "    k = 2\n",
    "    while(currentLSet != set([])):\n",
    "        largeSet[k-1] = currentLSet\n",
    "        currentLSet = joinSet(currentLSet, k)\n",
    "        currentCSet = returnItemsWithMinSupport(currentLSet,\n",
    "                                                transactionList,\n",
    "                                                minSupport,\n",
    "                                                freqSet)\n",
    "        currentLSet = currentCSet\n",
    "        k = k + 1\n",
    "\n",
    "    def getSupport(item):\n",
    "            \"\"\"local function which Returns the support of an item\"\"\"\n",
    "            return float(freqSet[item])/len(transactionList)\n",
    "\n",
    "    toRetItems = []\n",
    "    for key, value in list(largeSet.items()):\n",
    "        toRetItems.extend([(tuple(item), getSupport(item))\n",
    "                           for item in value])\n",
    "\n",
    "    toRetRules = []\n",
    "    for key, value in list(largeSet.items())[1:]:\n",
    "        for item in value:\n",
    "            _subsets = map(frozenset, [x for x in subsets(item)])\n",
    "            for element in _subsets:\n",
    "                remain = item.difference(element)\n",
    "                if len(remain) > 0:\n",
    "                    confidence = getSupport(item)/getSupport(element)\n",
    "                    if confidence >= minConfidence:\n",
    "                        toRetRules.append(((tuple(element), tuple(remain)),\n",
    "                                           confidence))\n",
    "    return toRetItems, toRetRules\n",
    "\n",
    "\n",
    "def printResults(items):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    for item, support in sorted(items, key=lambda item_support: item_support[1], reverse=True):\n",
    "        print(str(item), support)\n",
    "    #print (\"\\n------------------------ RULES:\")\n",
    "    #for rule, confidence in sorted(rules, key=lambda rule_confidence: rule_confidence[1]):\n",
    "        #pre, post = rule\n",
    "        #print (str(pre), str(post), confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility methods that can extract words after checking acceptable conditions and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:18:24.512391Z",
     "start_time": "2018-04-18T23:18:24.494930Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stem(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = word.replace(\"'\",\"\").replace('\"','').replace('.','')\n",
    "    word1 = stemmer.stem(word)\n",
    "    return word1\n",
    "\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool((2 <= len(word) <= 40) and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "        \n",
    "def get_terms(tree):\n",
    "    term = [ stem(w) for w in tree if acceptable_word(w) ]\n",
    "    yield term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a sets of nouns in each sentence for the apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:18:29.275651Z",
     "start_time": "2018-04-18T23:18:26.372662Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_fullreview = list()\n",
    "for index,ser_ in df_am.iterrows():\n",
    "    sent_fullreview.extend(ser_.tkn_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:18:30.753255Z",
     "start_time": "2018-04-18T23:18:30.299709Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_neg_review = list()\n",
    "for index,ser_ in df_am[df_am.rating <=2].iterrows():\n",
    "    sent_neg_review.extend(ser_.tkn_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:18:34.164636Z",
     "start_time": "2018-04-18T23:18:31.736125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_review = list()\n",
    "for index,ser_ in df_am[df_am.rating >=3].iterrows():\n",
    "    sent_review.extend(ser_.tkn_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:20:34.446089Z",
     "start_time": "2018-04-18T23:18:35.183603Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['la', 'girl', 'eye', 'shadow'], ['color', 'palett', 'color', 'work', 'day', 'color', 'night', 'town'], ['la', 'girl', 'eye', 'shadow', 'mascara', 'lash', 'hd', 'hairgen', 'volum', 'fiber', 'mascara', 'extrem', 'high', 'definit', 'lash', 'eye', 'pop'], ['combo'], ['nude'], ['drug', 'store', 'brand', 'brand'], ['pro', 'matt', 'color', 'base', 'blend'], ['surpris', 'pallet', 'shimmer', 'color', 'alik'], [], ['brush'], ['day'], ['suggest', 'color', 'matt'], ['color'], ['mix', 'shimmer', 'matt', 'shadow'], ['pigment', 'price'], [], ['month'], ['color'], ['color'], ['anyon']]\n"
     ]
    }
   ],
   "source": [
    "def is_noun(n):\n",
    "    if n=='NN' or n=='NNS' or n=='NNP' or n=='NNPS':\n",
    "        return True\n",
    "\n",
    "revset=[]\n",
    "for line in sent_fullreview:\n",
    "    a = nltk.word_tokenize(line)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(a) if is_noun(pos)] \n",
    "    terms = get_terms(nouns)\n",
    "        \n",
    "    for term in terms:   \n",
    "        tempset=[]\n",
    "        for word in term:\n",
    "            tempset.append(word)\n",
    "        revset.append(tempset)\n",
    "print(revset[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the apriori alogrithm on the noun sets to identify the most frequent nouns. We also need to mention the minimum support and the confidence. The resulting output items are the most common features in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:22:24.018132Z",
     "start_time": "2018-04-18T23:20:35.535210Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('color',) 0.18222083711090964\n",
      "('product',) 0.09679094892716833\n",
      "('eye',) 0.07644870051375038\n",
      "('shadow',) 0.06267943487458447\n",
      "('palett',) 0.055936461166515565\n",
      "('day',) 0.0447265034753702\n",
      "('eyeshadow',) 0.04248828951344817\n",
      "('price',) 0.038342399516470234\n",
      "('primer',) 0.03650083106678755\n",
      "('makeup',) 0.034102070111816256\n",
      "('eye', 'shadow') 0.03379042006648534\n",
      "('shade',) 0.0264052583862194\n",
      "('time',) 0.02591417346630402\n",
      "('brush',) 0.023808174675128437\n",
      "('skin',) 0.022873224539135692\n",
      "('look',) 0.019492293744333637\n",
      "('qualiti',) 0.01928452704744636\n",
      "('pallet',) 0.019142867935932305\n",
      "('pigment',) 0.01828346932608039\n",
      "('powder',) 0.01746184647929888\n",
      "('lot',) 0.01732963130855243\n",
      "('great',) 0.017282411604714415\n",
      "('eye', 'color') 0.013958144454517981\n",
      "('color', 'palett') 0.013835373224539135\n",
      "('way',) 0.013731489876095498\n",
      "('bit',) 0.01368427017225748\n",
      "('matt',) 0.012702100332426716\n",
      "('eyebrow',) 0.012588773043215472\n",
      "('brow',) 0.01257932910244787\n",
      "('conceal',) 0.012163795708673316\n",
      "('applic',) 0.011833257781807192\n",
      "('base',) 0.011804925959504381\n",
      "('face',) 0.011030522816560894\n",
      "('review',) 0.010747204593532789\n",
      "('packag',) 0.010728316711997582\n",
      "('brand',) 0.010511106074342701\n"
     ]
    }
   ],
   "source": [
    "items, rules = runApriori(revset, 0.01, 0.05)\n",
    "printResults(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liu Hu lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the Liu and Hu Opinion Lexicon to classify each sentence as positive, negitive or neutral. This is a simple algorithm which counts the number of positive and negitive words in a sentence and classifies the sentence accordingly. If any negation words like no or not are present, the sentiment is reversed.\n",
    "\n",
    "This process may take a couple of hours based on the size of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T00:28:49.357830Z",
     "start_time": "2018-04-19T00:28:49.356081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T00:28:30.627627Z",
     "start_time": "2018-04-19T00:28:30.593492Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def custom_liu_hu_lexicon(sentence):\n",
    "    '''Takes in a sentence and returns the sentiment of the sentence by counting the no of positive and negitive \n",
    "    and negitive words and by reversing the sentiment if the words NO or NOT are present\n",
    "    '''\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    x = list(range(len(tokenized_sent))) \n",
    "    y = []\n",
    "    isNegation = False\n",
    "    negationWords = ['no','not','never','none','hardly','rarely','scarcely','']\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "            y.append(1) # positive\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "            y.append(-1) # negative\n",
    "        else:\n",
    "            y.append(0) # neutral\n",
    "            \n",
    "        if word in negationWords:\n",
    "            isNegation = True\n",
    "\n",
    "    if pos_words > neg_words and isNegation==True:\n",
    "        return 'neg'\n",
    "    elif pos_words > neg_words:\n",
    "        return 'pos'\n",
    "    elif pos_words < neg_words and isNegation==True:\n",
    "        return 'pos'\n",
    "    elif pos_words < neg_words:\n",
    "        return 'neg'\n",
    "    elif pos_words == neg_words:\n",
    "        return 'neutral'\n",
    "\n",
    "neutral_review=[]\n",
    "positive_review=[]\n",
    "negative_review=[]\n",
    "for sentence in sent_review:\n",
    "    for i in items:\n",
    "        if i[0][0] in sentence:\n",
    "            #print(i[0][0] +\"--\" + sentence)\n",
    "            x=custom_liu_hu_lexicon(sentence)\n",
    "            if(x==\"pos\"):\n",
    "                positive_review.append(sentence)\n",
    "            elif(x==\"neg\"):\n",
    "                negative_review.append(sentence)\n",
    "            else:\n",
    "                neutral_review.append(sentence)\n",
    "            break\n",
    "\n",
    "for sentence in sent_neg_review:\n",
    "    for i in items:\n",
    "        if i[0][0] in sentence:\n",
    "            #print(i[0][0] +\"--\" + sentence)\n",
    "            negative_review.append(sentence)\n",
    "            break\n",
    "print('done')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T19:38:59.620933Z",
     "start_time": "2018-04-18T19:38:59.616989Z"
    }
   },
   "source": [
    "## Negative review Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:12:15.562400Z",
     "start_time": "2018-04-11T21:12:15.557797Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They are highly pigmented which took me by surprise because the pallet is so inexpensive.CONS: The shimmer colors are all very much alike.',\n",
       " 'Only suggestion I would make is that the colors be more distinguished from one another and that there be a dark matte.',\n",
       " 'A mix of shimmer and matte shadows.',\n",
       " 'Colors are very beautiful and pigmented, but not too much.',\n",
       " 'It\\'s great for me for school and everyday looks and I would consider it an amazing \"everyday pallete\" A lot of these colors are good for packing onto the lid.Pros:-Pigmented (not the white, but good as a base color)-Smooth and blendable-Natural colors-Affordable!-Sleek and easy packaging with magnetic close and shut-Small and thin mirror along the top inside and comes with a brush-Lasting power is pretty goodCons:-The shadows can pick up some chalk-Beware of fallout, but it\\'s not as bad as some other eyeshadows I\\'ve tried.-Can smudge-Colors are very similar, some almost feel like the same colors-Not many mattes or transition shadesWho would I recommend this to?-People who don\\'t have too much experience with makeup would love this as a first pallete I think, but even people who do have multiple palletes may love to add this to there collection-People who go to school-Casual makeup wearers-Any age-People more on the lighter side (may look ashy and be a little cool toned for darker skin)I honestly feel like almost anyone can enjoy this pallete or at least some of the shades in it.']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_review[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T05:08:59.464981Z",
     "start_time": "2018-04-11T05:08:59.461389Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35660"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T05:05:31.009599Z",
     "start_time": "2018-04-11T05:05:31.005186Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I absolutely love LA Girl eye shadow.', 'The color palette is just perfect, with mild colors for work and day wear along with bold colors for a night on the town.', 'I use the LA Girl eye shadow along with this mascara\\xa0Lavish Lash HD by Hairgenics - Ultra-Premium Volumizing Fiber Mascara for Extreme High Definition Lashes\\xa0and it really makes my eyes pop.', 'I buy what I like regardless of who makes it!PROS: the matte colors are perfect for base and blending.', 'The brush it comes with is useless.Overall I like it and would recommend.', 'Really pretty colors.', 'I love the rosy colors the most.', 'I bet they look great on anyone.', \"I haven't really used this a lot, but the colours look good.\", 'I was pleasantly surprised by these eye shadows.']\n"
     ]
    }
   ],
   "source": [
    "print(positive_review[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dump our results into a file so that we need not repeat this time consuming process again if we need those results later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T19:39:15.643835Z",
     "start_time": "2018-04-18T19:39:15.639414Z"
    }
   },
   "source": [
    "## Save and Load section in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T05:09:24.991368Z",
     "start_time": "2018-04-11T05:09:24.956095Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(PATH+'all_pos_rating_op5.txt', 'wb') as fp:  \n",
    "    pickle.dump(positive_review, fp)\n",
    "\n",
    "print('done')    \n",
    "    \n",
    "with open(PATH+'all_neg_rating_op5.txt', 'wb') as fp:  \n",
    "    pickle.dump(negative_review, fp)\n",
    "\n",
    "print('done')\n",
    "\n",
    "with open(PATH+'all_neutral_rating_op5.txt', 'wb') as fp:  \n",
    "    pickle.dump(neutral_review, fp)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fetch the dumped data back into python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T21:54:06.175712Z",
     "start_time": "2018-04-18T21:54:02.408297Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I absolutely love LA Girl eye shadow.', 'The color palette is just perfect, with mild colors for work and day wear along with bold colors for a night on the town.', 'I use the LA Girl eye shadow along with this mascara\\xa0Lavish Lash HD by Hairgenics - Ultra-Premium Volumizing Fiber Mascara for Extreme High Definition Lashes\\xa0and it really makes my eyes pop.', 'I buy what I like regardless of who makes it!PROS: the matte colors are perfect for base and blending.', 'The brush it comes with is useless.Overall I like it and would recommend.']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pos_sentences=[];neg_sentences=[];neutral_sentences=[]\n",
    "with open (PATH+'all_pos_rating_op5.txt', 'rb') as fp:\n",
    "    pos_sentences = pickle.load(fp)\n",
    "with open (PATH+'all_neg_rating_op5.txt', 'rb') as fp:\n",
    "    neg_sentences = pickle.load(fp)\n",
    "with open (PATH+'all_neutral_rating_op5.txt', 'rb') as fp:\n",
    "    neutral_sentences = pickle.load(fp)\n",
    "\n",
    "print(str(pos_sentences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After classifying the reviews as positive and negitive, we tokenize those reviews into words and apply part of speech(POS) tagging on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T00:22:17.294703Z",
     "start_time": "2018-04-19T00:22:17.292287Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are highly pigmented which took me by surprise because the pallet is so inexpensive.CONS: The shimmer colors are all very much alike.\n",
      "Only suggestion I would make is that the colors be more distinguished from one another and that there be a dark matte.\n",
      "A mix of shimmer and matte shadows.\n"
     ]
    }
   ],
   "source": [
    "for i in neg_sentences[5:8]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T19:39:48.879154Z",
     "start_time": "2018-04-18T19:39:48.874517Z"
    }
   },
   "source": [
    "## Pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T21:55:44.721714Z",
     "start_time": "2018-04-18T21:54:30.701020Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('absolutely', 'RB'), ('love', 'VBP'), ('LA', 'NNP'), ('Girl', 'NNP'), ('eye', 'NN'), ('shadow', 'NN'), ('.', '.')], [('The', 'DT'), ('color', 'NN'), ('palette', 'NN'), ('is', 'VBZ'), ('just', 'RB'), ('perfect', 'JJ'), (',', ','), ('with', 'IN'), ('mild', 'JJ'), ('colors', 'NNS'), ('for', 'IN'), ('work', 'NN'), ('and', 'CC'), ('day', 'NN'), ('wear', 'VBP'), ('along', 'IN'), ('with', 'IN'), ('bold', 'JJ'), ('colors', 'NNS'), ('for', 'IN'), ('a', 'DT'), ('night', 'NN'), ('on', 'IN'), ('the', 'DT'), ('town', 'NN'), ('.', '.')], [('I', 'PRP'), ('use', 'VBP'), ('the', 'DT'), ('LA', 'NNP'), ('Girl', 'NNP'), ('eye', 'NN'), ('shadow', 'NN'), ('along', 'IN'), ('with', 'IN'), ('this', 'DT'), ('mascara', 'NN'), ('Lavish', 'JJ'), ('Lash', 'NNP'), ('HD', 'NNP'), ('by', 'IN'), ('Hairgenics', 'NNP'), ('-', ':'), ('Ultra-Premium', 'JJ'), ('Volumizing', 'NNP'), ('Fiber', 'NNP'), ('Mascara', 'NNP'), ('for', 'IN'), ('Extreme', 'NNP'), ('High', 'NNP'), ('Definition', 'NNP'), ('Lashes', 'NNP'), ('and', 'CC'), ('it', 'PRP'), ('really', 'RB'), ('makes', 'VBZ'), ('my', 'PRP$'), ('eyes', 'NNS'), ('pop', 'NN'), ('.', '.')], [('I', 'PRP'), ('buy', 'VBP'), ('what', 'WP'), ('I', 'PRP'), ('like', 'VBP'), ('regardless', 'RB'), ('of', 'IN'), ('who', 'WP'), ('makes', 'VBZ'), ('it', 'PRP'), ('!', '.'), ('PROS', 'NNS'), (':', ':'), ('the', 'DT'), ('matte', 'NN'), ('colors', 'NNS'), ('are', 'VBP'), ('perfect', 'JJ'), ('for', 'IN'), ('base', 'NN'), ('and', 'CC'), ('blending', 'NN'), ('.', '.')], [('The', 'DT'), ('brush', 'NN'), ('it', 'PRP'), ('comes', 'VBZ'), ('with', 'IN'), ('is', 'VBZ'), ('useless.Overall', 'JJ'), ('I', 'PRP'), ('like', 'VBP'), ('it', 'PRP'), ('and', 'CC'), ('would', 'MD'), ('recommend', 'VB'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "pos_tokens=[];pos_tokens_postagged=[];neg_tokens=[];neg_tokens_postagged=[];neut_tokens=[];neut_tokens_postagged=[];\n",
    "'''Tokenize the sentences into words'''\n",
    "for sent in pos_sentences: \n",
    "    pos_tokens.append(nltk.word_tokenize(sent))    \n",
    "for sent in neg_sentences:\n",
    "    neg_tokens.append(nltk.word_tokenize(sent))    \n",
    "for sent in neutral_sentences:\n",
    "    neut_tokens.append(nltk.word_tokenize(sent))\n",
    "\n",
    "'''Apply Part of speech tagging for the tokenized words'''\n",
    "for sent in pos_tokens:    \n",
    "    pos_tokens_postagged.append(nltk.tag.pos_tag(sent))\n",
    "for sent in neg_tokens:    \n",
    "    neg_tokens_postagged.append(nltk.tag.pos_tag(sent))\n",
    "for sent in neut_tokens:    \n",
    "    neut_tokens_postagged.append(nltk.tag.pos_tag(sent))  \n",
    "    \n",
    "print(pos_tokens_postagged[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Feature by POS tag\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After POS tagging is done, we need to extract features that are opinion phrases using nltk chunking.\n",
    "\n",
    "We also remove the stop words, filter out words that are too small and or that are too big, stem and lemmatize the words so that we can find the repeting phrases accurately\n",
    "\n",
    "**This paper has been used to identify the chunking patterns:**\n",
    "Su Su Htay and Khin Thidar Lynn, “Extracting Product Features and Opinion Words Using Pattern Knowledge in Customer Reviews,” The Scientific World Journal, vol. 2013, Article ID 394758, 5 pages, 2013. doi:10.1155/2013/394758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T22:08:57.884028Z",
     "start_time": "2018-04-18T22:08:57.661145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lem_word_mapping={}\n",
    "\n",
    "gram = r\"\"\"       \n",
    "    P1:{<JJ><NN|NNS>}\n",
    "    P2:{<JJ><NN|NNS><NN|NNS>}\n",
    "    P3:{<RB|RBR|RBS><JJ>}\n",
    "    P4:{<RB|RBR|RBS><JJ|RB|RBR|RBS><NN|NNS>}\n",
    "    P5:{<RB|RBR|RBS><VBN|VBD>}\n",
    "    P6:{<RB|RBR|RBS><RB|RBR|RBS><JJ>}\n",
    "    P7:{<VBN|VBD><NN|NNS>}\n",
    "    P8:{<VBN|VBD><RB|RBR|RBS>}\n",
    "\"\"\"\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label() in ['P1','P2','P3','P4','P5','P6','P7','P8']):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word1 = stemmer.stem(word)\n",
    "    word1 = lem.lemmatize(word1)    \n",
    "    if word!=word1:\n",
    "        lem_word_mapping[word1]=word\n",
    "    return word1\n",
    "\n",
    "def get_terms(tree):\n",
    "    \"\"\"Returns the words after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    term = [ normalise(w) for w in tree if acceptable_word(w) ]\n",
    "    yield term\n",
    "    \n",
    "def get_t_norm(tree):\n",
    "    \"\"\"Parse leaves in chunk and return after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        yield term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T22:10:58.215411Z",
     "start_time": "2018-04-18T22:10:15.312366Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mild color', 'bold color', 'pretti color', 'rosi color', 'realli use', 'pleasantli surpris', 'great idea', 'quick swipe', 'favorit eyeshadow', 'perfect shade', 'green eye', 'mild color', 'bold color', 'pretti color', 'rosi color', 'realli use', 'pleasantli surpris', 'great idea', 'quick swipe', 'favorit eyeshadow', 'perfect shade', 'green eye', 'popular nude', 'great product', 'truli love', 'correct term', 'high hope', 'bright room', 'warm red/yellow', 'mix skin', 'wide array', 'natur shade', 'great start', 'basic colour', 'foundation/cheek/ey color', 'profession ea', 'profession look', 'pigment shade', 'compact mirror', 'high-end eyeshadow', 'afford cost', 'maximum qualiti', 'pigment glimmer', 'gorgeou look', 'complimentari light', 'extrem fun', 'desir look', 'favorit pallet', 'almost ident', 'great shadow']\n"
     ]
    }
   ],
   "source": [
    "def extractOpinionPhrases(posTaggedData):\n",
    "    '''Extract noun phrases from part of speech tagged tokenized words'''\n",
    "    output=[]\n",
    "    for tup in posTaggedData:\n",
    "        chunk = nltk.RegexpParser(gram)\n",
    "        tr = chunk.parse(tup)\n",
    "        term = get_t_norm(tr)\n",
    "\n",
    "        for ter in term:    \n",
    "            wordConcat=\"\"\n",
    "            for word in ter:\n",
    "                if wordConcat==\"\":\n",
    "                    #Replace good, wonderful and awesome with great\n",
    "                    wordConcat = wordConcat + word.replace(\"good\",\"great\").replace(\"wonderful\",\"great\").replace(\"awesome\",\"great\").replace(\"awesom\",\"great\")\n",
    "                else:\n",
    "                    wordConcat = wordConcat + \" \" +  word\n",
    "            if(len(ter)>1):\n",
    "                output.append(wordConcat) \n",
    "    return output\n",
    "\n",
    "ExtractedWords_pos = extractOpinionPhrases(pos_tokens_postagged)\n",
    "ExtractedWords_neg = extractOpinionPhrases(neg_tokens_postagged)\n",
    "        \n",
    "print(ExtractedWords_pos[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the opinion words and features, now we identify the most common opinions by using frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:00:16.065923Z",
     "start_time": "2018-04-18T23:00:15.985499Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great product', 983), ('great color', 502), ('great price', 478), ('beauti color', 372), ('great qualiti', 310), ('nice color', 213), ('great pigment', 199), ('long time', 192), ('highli pigment', 137), ('natur look', 134), ('neutral color', 127), ('great deal', 125), ('great buy', 118), ('first time', 118), ('ever use', 117), ('differ color', 116), ('great palett', 109), ('long way', 109), ('littl bit', 109), ('eyeshadow primer', 108), ('pretti color', 107), ('excel product', 105), ('bright color', 100), ('mani color', 97), ('nice product', 95), ('pretti good', 94), ('long last', 88), ('perfect condit', 87), ('high end', 87), ('favorit eye', 87), ('differ shade', 83), ('realli good', 83), ('great primer', 82), ('great eye', 82), ('great base', 81), ('great valu', 81), ('fair skin', 78), ('sensit skin', 76), ('blue eye', 75), ('natur color', 73), ('reason price', 72), ('skin tone', 71), ('nice pigment', 70), ('high qualiti', 70), ('realli nice', 70), ('perfect color', 70), ('well pigment', 67), ('great coverag', 66), ('great condit', 63), ('great eyeshadow', 63)]\n",
      "----\n",
      "[('great product', 89), ('urban decay', 84), ('littl bit', 79), ('first time', 68), ('dark brown', 66), ('sensit skin', 66), ('oili skin', 65), ('great qualiti', 65), ('great review', 61), ('great color', 55), ('high end', 49), ('long time', 47), ('dark circl', 42), ('came broken', 42), ('never use', 39), ('highli pigment', 39), ('differ color', 38), ('pretti color', 35), ('great pigment', 34), ('eyelid primer', 34), ('expens brand', 33), ('eyeshadow primer', 33), ('brown hair', 32), ('dark skin', 31), ('pale skin', 30), ('bubbl wrap', 29), (\"n't last\", 29), ('beauti color', 29), ('nice color', 29), ('great coverag', 28), ('mani color', 27), ('great price', 27), ('high qualiti', 27), ('complet broken', 26), ('dri skin', 26), ('much product', 25), ('differ shade', 24), ('great primer', 24), ('long last', 24), ('skin tone', 24), ('broken eyeshadow', 24), ('littl darker', 23), ('great thing', 22), ('high hope', 22), ('realli want', 22), ('next day', 22), ('great deal', 22), ('hard time', 22), ('coastal scent', 21), ('bad review', 21)]\n"
     ]
    }
   ],
   "source": [
    "freqdist_neg = nltk.FreqDist(word for word in ExtractedWords_neg)\n",
    "mc_neg = freqdist_neg.most_common()\n",
    "freqdist_pos = nltk.FreqDist(word for word in ExtractedWords_pos)\n",
    "mc_pos = freqdist_pos.most_common()\n",
    "\n",
    "print(mc_pos[:50])\n",
    "print(\"----\")\n",
    "print(mc_neg[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then unlemmatize and unstem the opinion words extracted above using the dictionary we created which mapped the words raw form with the stemmed and lemmatized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:12:03.584784Z",
     "start_time": "2018-04-18T23:11:56.232645Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive Opinon phrases:\n",
      "[('greate product', 983), ('greate color', 502), ('greate priced', 478), ('beautifully color', 372), ('greate quality', 310), ('nicely color', 213), ('greate pigmented', 199), ('long time', 192), ('highly pigmented', 137), ('natural looked', 134), ('neutral color', 127), ('greate deal', 125), ('greate buying', 118), ('first time', 118), ('ever used', 117), ('different color', 116), ('greate palette', 109), ('long way', 109), ('little bit', 109), ('eyeshadow primer', 108), ('pretty color', 107), ('excellent product', 105), ('bright color', 100), ('many color', 97), ('nicely product', 95), ('pretty good', 94), ('long lasting', 88), ('perfection condition', 87), ('high ended', 87), ('favorite eye', 87), ('different shade', 83), ('really good', 83), ('greate primer', 82), ('greate eye', 82), ('greate based', 81), ('greate value', 81), ('fair skin', 78), ('sensitive skin', 76), ('blue eye', 75), ('natural color', 73), ('reasonable priced', 72), ('skin toned', 71), ('nicely pigmented', 70), ('high quality', 70), ('really nicely', 70), ('perfection color', 70), ('well pigmented', 67), ('greate coverage', 66), ('greate condition', 63), ('greate eyeshadow', 63)]\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "def replacewords(mc):\n",
    "    newmc=[]\n",
    "    for a in mc:\n",
    "        newword=\"\";found=False;\n",
    "        for b in a[0].split():            \n",
    "            for x in lem_word_mapping:\n",
    "                #print(x)\n",
    "                #print(b)\n",
    "                if b==x:\n",
    "                    found=True\n",
    "                    sing=(lem_word_mapping[x] if p.singular_noun(lem_word_mapping[x])==False else p.singular_noun(lem_word_mapping[x]))\n",
    "                    if newword==\"\":\n",
    "                        newword = newword + sing\n",
    "                    else:\n",
    "                        newword = newword + \" \" +  sing\n",
    "            if found==False:\n",
    "                if newword==\"\":\n",
    "                    newword = newword + b\n",
    "                else:\n",
    "                    newword = newword + \" \" +  b\n",
    "                    #print(newword)\n",
    "        newmc.append((newword,a[1]))\n",
    "    return newmc\n",
    "\n",
    "final_neg = replacewords(mc_neg)\n",
    "final_pos = replacewords(mc_pos)\n",
    "\n",
    "print(\"Postive Opinon phrases:\")\n",
    "print(final_pos[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T23:12:04.614224Z",
     "start_time": "2018-04-18T23:12:04.610611Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negitive Opinion phrases:\n",
      "[('greate product', 89), ('urban decay', 84), ('little bit', 79), ('first time', 68), ('darknes brown', 66), ('sensitive skin', 66), ('oily skin', 65), ('greate quality', 65), ('greate review', 61), ('greate color', 55), ('high ended', 49), ('long time', 47), ('darknes circle', 42), ('came broken', 42), ('never used', 39), ('highly pigmented', 39), ('different color', 38), ('pretty color', 35), ('greate pigmented', 34), ('eyelid primer', 34), ('expensive brand', 33), ('eyeshadow primer', 33), ('brown hair', 32), ('darknes skin', 31), ('pale skin', 30), ('bubbly wrap', 29), (\"n't lasting\", 29), ('beautifully color', 29), ('nicely color', 29), ('greate coverage', 28), ('many color', 27), ('greate priced', 27), ('high quality', 27), ('complete', 26), ('dry skin', 26), ('much product', 25), ('different shade', 24), ('greate primer', 24), ('long lasting', 24), ('skin toned', 24), ('broken eyeshadow', 24), ('little', 23), ('greate thing', 22), ('high hope', 22), ('really wanted', 22), ('next day', 22), ('greate deal', 22), ('hard time', 22), ('coastal scent', 21), ('bad review', 21)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Negitive Opinion phrases:\")\n",
    "print(final_neg[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T00:48:07.173131Z",
     "start_time": "2018-04-19T00:48:07.170144Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_pos_p = list()\n",
    "for x,_ in final_pos:\n",
    "    ls_pos_p.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T00:48:59.840994Z",
     "start_time": "2018-04-19T00:48:59.170922Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_neg_clean = list()\n",
    "for neg_p, fre in final_neg:\n",
    "    if neg_p not in ls_pos_p:\n",
    "        final_neg_clean.append((neg_p,fre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for each feature, if the feature is contained in the most common opinion phrases, we extract the opinions corresponding to that feature and opinion phrase. Only first 3 opinions for corresponding feature and first 10 corresponding opinions are fetched here as the dataset is huge. You may remove the count and the break statements if you want all the corresponding reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T03:26:41.474461Z",
     "start_time": "2018-04-19T03:26:41.458355Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finResult(itemArr, opinionPhrases, sentenceArr):\n",
    "    for item,support in sorted(items, key=lambda item_support: item_support[1], reverse=True):\n",
    "        count=0\n",
    "        print(\"----------\"+item[0]+\"----------\")\n",
    "        for phrase,freq in sorted(opinionPhrases, key=lambda phrase_freq: phrase_freq[1], reverse=True): \n",
    "            pcount=0\n",
    "            if normalise(item[0]) in normalise(phrase):\n",
    "                count+=1\n",
    "                print(\"---\"+phrase+\"---\")\n",
    "#                 for l in sentenceArr:\n",
    "#                     if normalise(phrase) in normalise(l):   \n",
    "#                         for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:]):\n",
    "#                             #print(b[0]+\" \"+b[1])\n",
    "#                             if normalise(b[0])==normalise(item[0]):\n",
    "#                                 print(l.replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\"))\n",
    "#                                 pcount+=1\n",
    "#                                 break\n",
    "#                             elif (normalise(b[0])+\" \"+normalise(b[1]))==normalise(item[0]):\n",
    "#                                 print(l.replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\"))\n",
    "#                                 pcount+=1\n",
    "#                                 break\n",
    "#                         if pcount==1:\n",
    "#                             break                \n",
    "            if count==3:\n",
    "                break \n",
    "                \n",
    "# posSentStr = \"\"\n",
    "# posSentStr = posSentStr.join(pos_sentences)\n",
    "# posTokenSentences = sent_tokenizer1.tokenize(posSentStr)\n",
    "# finResult(items, mc_pos, posTokenSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T03:26:45.771821Z",
     "start_time": "2018-04-19T03:26:42.940791Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------color----------\n",
      "---intense color---\n",
      "---cracked color---\n",
      "---shimmering color---\n",
      "----------product----------\n",
      "---real product---\n",
      "---fake product---\n",
      "---wrong product---\n",
      "----------eye----------\n",
      "---invisible eyebrow---\n",
      "---eye watered---\n",
      "---ordered eyeshadow---\n",
      "----------shadow----------\n",
      "---ordered eyeshadow---\n",
      "---eyeshadow disk---\n",
      "---eyeshadow pencil---\n",
      "----------palett----------\n",
      "---bought palette---\n",
      "---broken palette---\n",
      "---brown palette---\n",
      "----------day----------\n",
      "---came yesterday---\n",
      "---30-day returned---\n",
      "---hot yesterday---\n",
      "----------eyeshadow----------\n",
      "---ordered eyeshadow---\n",
      "---eyeshadow disk---\n",
      "---eyeshadow pencil---\n",
      "----------price----------\n",
      "---super priceye---\n",
      "---overpriced fake---\n",
      "---low pricecon---\n",
      "----------primer----------\n",
      "---primer/shadow based---\n",
      "---nyx primer/base---\n",
      "---primer thank---\n",
      "----------makeup----------\n",
      "---expensive makeup---\n",
      "---makeup application---\n",
      "---makeup online---\n",
      "----------eye----------\n",
      "---invisible eyebrow---\n",
      "---eye watered---\n",
      "---ordered eyeshadow---\n",
      "----------shade----------\n",
      "---shimmering shades.picture---\n",
      "---right shade..---\n",
      "---3-4 shade---\n",
      "----------time----------\n",
      "---long time.also---\n",
      "---nighttime emphasis.you---\n",
      "---next time.i---\n",
      "----------brush----------\n",
      "---synthetic brush---\n",
      "---cheap brush---\n",
      "---free brush---\n",
      "----------skin----------\n",
      "---skin break---\n",
      "---skin reaction---\n",
      "---skin issue---\n",
      "----------look----------\n",
      "---intense looked---\n",
      "---looked nothing---\n",
      "---usable.i looked---\n",
      "----------qualiti----------\n",
      "----------pallet----------\n",
      "---real pallete---\n",
      "---top pallette---\n",
      "---genuine pallette---\n",
      "----------pigment----------\n",
      "---pigmented eyelid---\n",
      "---pigmented wisely---\n",
      "---pigmented otherwise---\n",
      "----------powder----------\n",
      "---really powdery---\n",
      "---powdery feeling---\n",
      "---powder based---\n",
      "----------lot----------\n",
      "---tried lot---\n",
      "---ruined clothe---\n",
      "---nicely lot---\n",
      "----------great----------\n",
      "---greate rating---\n",
      "---greate results.the---\n",
      "---greate ingredient---\n",
      "----------eye----------\n",
      "---invisible eyebrow---\n",
      "---eye watered---\n",
      "---ordered eyeshadow---\n",
      "----------color----------\n",
      "---intense color---\n",
      "---cracked color---\n",
      "---shimmering color---\n",
      "----------way----------\n",
      "---alway ended---\n",
      "---way.thi product---\n",
      "---hard way---\n",
      "----------bit----------\n",
      "---bit skeptical---\n",
      "---orbital bone---\n",
      "---bit curiou---\n",
      "----------matt----------\n",
      "---wanted matte---\n",
      "---pretty matte---\n",
      "---creamy matte---\n",
      "----------eyebrow----------\n",
      "---invisible eyebrow---\n",
      "---nyx eyebrow---\n",
      "---itmy eyebrow---\n",
      "----------brow----------\n",
      "---invisible eyebrow---\n",
      "---brownish toned---\n",
      "---brown palette---\n",
      "----------conceal----------\n",
      "---horrible concealer---\n",
      "---bought concealer---\n",
      "---stick concealer---\n",
      "----------applic----------\n",
      "---application wandcon---\n",
      "---application.the packaging---\n",
      "----------base----------\n",
      "---greate basefeel---\n",
      "---based coat---\n",
      "---cream-based makeup---\n",
      "----------face----------\n",
      "---faced product---\n",
      "---in-your-face-neon-violet purple---\n",
      "---face.five star---\n",
      "----------review----------\n",
      "---negative review---\n",
      "---top review---\n",
      "---fake review---\n",
      "----------packag----------\n",
      "---cheap packaging---\n",
      "---poor packaging---\n",
      "---also packaging---\n",
      "----------brand----------\n",
      "---cheap brand---\n",
      "---variou brand---\n",
      "---known brand---\n"
     ]
    }
   ],
   "source": [
    "negSentStr = \"\"\n",
    "negSentStr = negSentStr.join(neg_sentences)\n",
    "negTokenSentences = sent_tokenizer1.tokenize(negSentStr)\n",
    "finResult(items, final_neg_clean, negTokenSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 555,
   "position": {
    "height": "40px",
    "left": "1485px",
    "right": "20px",
    "top": "120px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
